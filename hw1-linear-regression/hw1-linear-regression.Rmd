---
title: "2022 Spring - Econometrics - Problem Set 1"
author: "B09303004 何雨忻"
date: "March 31, 2022"
output:
    pdf_document:
        latex_engine: "xelatex"
papersize: a4
header-includes:
    - \DeclareMathOperator{\Var}{Var}
    - \DeclareMathOperator{\Cov}{Cov}
    - \usepackage{xeCJK}
    - \usepackage{unicode-math}
    - \setmainfont{Charter}
    - \setCJKmainfont{Noto Serif CJK TC}
---
```{=latex}
\begin{enumerate}
    \item{
        From Model A: 
        \begin{align*}
                        y_i &= \beta_0 + \beta_1 x_i + \epsilon_i \\
            \Rightarrow \epsilon_i &= y_i - \beta_0 - \beta_1 x_i \\
            \Rightarrow \sum_{i=1}^{n} \epsilon_i^2 &= \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i \right)^2 = Q_a
        \end{align*}
        
        Obtain first order conditions for minimizing residuals sum of square:
        \begin{align}
            \frac{\partial Q_a}{\partial \beta_0} &= -2 \sum(y_i - \beta_0 - \beta_1 x_i) = 0 \label{FOC1}\\
            \frac{\partial Q_a}{\partial \beta_1} &= -2 \sum x_i (y_i - \beta_0 - \beta_1 x_i) = 0 \label{FOC2}
        \end{align}
        
        From \eqref{FOC1}:
        \begin{gather}
            \sum y_i - n \Hat{\beta_0} - \hat{\beta_1} \sum x_i = 0 \nonumber \\
            \Rightarrow \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
            \label{beta0}
        \end{gather}
        
        From $\eqref{FOC2} - \bar{x} \eqref{FOC1}$:
        \begin{gather}
            \nonumber \sum (x_i - \bar{x})(y_i - \hat{\beta_0} - \hat{\beta}x_i) = 0 \\
            \nonumber \Rightarrow \sum (x_i - \bar{x})((y_i - \bar{y}) - \beta_1 (x_i - \bar{x})) = 0 \\
            \Rightarrow \hat{\beta_1} = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}
            \label{beta1}
        \end{gather}
        
        From Model B:
        \begin{align*}
                        y_i &= \gamma_0 + \gamma_1 (x_i - \bar{x}) + \epsilon_i \\
            \Rightarrow \epsilon_i &= y_i - \gamma_0 - \gamma_1 (x_i - \bar{x}) \\
            \Rightarrow \sum_{i=1}^{n} \epsilon_i^2 &= \sum_{i=1}^{n} \left( y_i - \gamma_0 - \gamma_1 (x_i - \bar{x}) \right)^2 = Q_b
        \end{align*}
        
        Obtain FOCs:
        \begin{align}
            \frac{\partial Q_b}{\partial \gamma_0} &= -2 \sum(y_i - \gamma_0 - \gamma_1 (x_i - \bar{x})) = 0 \label{FOC3}\\
            \frac{\partial Q_b}{\partial \gamma_1} &= -2 \sum (x_i - \bar{x}) (y_i - \gamma_0 - \gamma_1 (x_i - \bar{x})) = 0 \label{FOC4}
        \end{align}
        
        From \eqref{FOC4}:
        \begin{gather}
            \nonumber \sum y_i - n \hat{\gamma_0} - \hat{\gamma_1} \underbrace{\sum(x_i - \bar{x})}_{=0} = 0 \\
            \Rightarrow \hat{\gamma_0} = \bar{y} \label{gamma0}
        \end{gather}
        
        With $\eqref{FOC4} - \eqref{FOC3}$, we have:
        \begin{align}
            \nonumber &\sum(x_i - \bar{x})(y_i - \hat{\gamma_0} - \hat{\gamma_1}(x_i - \bar{x})) = 0 \\
            \nonumber \Rightarrow &\sum(x_i - \bar{x})(y_i - \bar{y} - \hat{\gamma_1}(x_i - \bar{x})) = 0 \\
            \nonumber \Rightarrow &\sum(x_i - \bar{x})(y_i - \bar{y}) - \hat{\gamma_1}\sum(x_i - \bar{x})^2 = 0 \\
            \Rightarrow &\hat{\gamma_1} = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}
            \label{gamma1}
        \end{align}
        
        As shown above, $\hat{\beta_1}$ and $\hat{\gamma_1}$ are identical and having the same variance, while $\hat{\beta_0} \neq \hat{\gamma_0}$ with difference in variance:
        \begin{align*}
            \Var(\hat{\beta_0}) - \Var(\hat{\gamma_0}) &= [\Var(\bar{y}) + \bar{x}^2\Var(\hat{\beta_1}) - 2\bar{x} \Cov(\bar{y}, \hat{\beta_1})] - \Var(\bar{y}) \\
            &= \bar{x}^2\Var(\hat{\beta_1}) - 2\bar{x} \Cov\left(\frac{1}{n} \sum y_i, \frac{\sum(x_i - \bar{x})y_i}{\sum(x_i - \bar{x})^2}\right) \\
            &= \bar{x}^2\Var(\hat{\beta_1}) - 2\bar{x} \frac{1}{n} \frac{1}{\sum(x_i - \bar{x})^2} \Cov\left(\sum y_i, \sum(x_i - \bar{x})y_i\right) \\
            &= \bar{x}^2\Var(\hat{\beta_1}) - \frac{2\bar{x}}{n \sum(x_i - \bar{x})^2} \sum(x_i - \bar{x}) \Var\left(y_i\right) \\
            &= \bar{x}^2\Var(\hat{\beta_1}) - \frac{2\bar{x} \sigma_y^2}{n \sum(x_i - \bar{x})^2} \underbrace{\sum(x_i - \bar{x})}_{=0} \\
            &= \bar{x}^2\Var(\hat{\beta_1}) > 0
        \end{align*}
        Thus, variance of estimator $\hat{\beta_0}$ is larger.
    }
    
    \item {
        For a linear model without intercept, we have FOC for minimizing residual:
        \begin{align}
            \frac{\partial Q_a}{\partial \beta_1} &= -2 \sum x_i (y_i - \beta_1 x_i) = 0 \label{FOC5}
        \end{align}
        
        which could be derived into an algebraic property of OLS estimator:
        \begin{align*}
            \sum_{i=1}^{n} x_i (y_i - \hat{\beta_1} x_i) = \sum_{i=1}^{n} x_i \hat{\epsilon_i} = 0
        \end{align*}
        
        By definition,
        \begin{align*}
            \text{SST} &= \sum_{i=1}^{n} (y_i - \bar{y})^2 \\
                       &= \sum_{i=1}^{n} (\hat{y_i} - \bar{y} + \hat{\epsilon_i})^2 \\
                       &= \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2 + \sum_{i=1}^{n} (\hat{\epsilon_i})^2 + 2 \sum_{i=1}^{n} (\hat{y_i} - \bar{y})(\hat{\epsilon_i}) \\
                       &= \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2 + \sum_{i=1}^{n} (\hat{\epsilon_i})^2 + 2 \sum_{i=1}^{n} (\hat{\beta_1} x_i - \bar{y})(\hat{\epsilon_i}) \\
                       &= \underbrace{\sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2}_{\text{SSE}} + \underbrace{\sum_{i=1}^{n} (\hat{\epsilon_i})^2}_{\text{SSR}} + 2 \hat{\beta_1} \underbrace{\sum_{i=1}^{n} (x_i \hat{\epsilon_i})}_{=0} - 2 \bar{y} \sum_{i=1}^{n} \hat{\epsilon_i} \\
                       &= \text{SSE} + \text{SSR} - 2 \bar{y} \sum_{i=1}^{n} \hat{\epsilon_i}
        \end{align*}
        
        Therefore, for linear model without intercept, SSR isn't necessarily equals to SSE + SSR
    }
    \item {
        Construct variables in matrix form:
        \[
            \mathbf{y} = \begin{bmatrix}
                    1 \\ 0 \\ 2 \\ 1 \\ 3 \\ 5
                \end{bmatrix}, 
            \mathbf{x} = \begin{bmatrix}
                    1 & 0 & 0 \\
                    1 & 0 & 1 \\
                    1 & 0 & 2 \\
                    1 & 1 & 0 \\
                    1 & 1 & 1 \\
                    1 & 1 & 2
                \end{bmatrix},
            \hat{\beta} = \begin{bmatrix}
                    \hat{\beta_0} \\ \hat{\beta_1} \\ \hat{\beta_2}
                    \end{bmatrix}
        \]
    
        \begin{enumerate}
            \item {
                \begin{align*}
                    \hat{\beta} &= (\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
                          &= \left(
                             \begin{bmatrix}
                                  1 & 1 & 1 & 1 & 1 & 1 \\
                                  0 & 0 & 0 & 1 & 1 & 1 \\
                                  0 & 1 & 2 & 0 & 1 & 2 \\
                             \end{bmatrix}
                             \begin{bmatrix}
                                1 & 0 & 0 \\
                                1 & 0 & 1 \\
                                1 & 0 & 2 \\
                                1 & 1 & 0 \\
                                1 & 1 & 1 \\
                                1 & 1 & 2
                             \end{bmatrix}
                             \right)^{-1}
                             \begin{bmatrix}
                                  1 & 1 & 1 & 1 & 1 & 1 \\
                                  0 & 0 & 0 & 1 & 1 & 1 \\
                                  0 & 1 & 2 & 0 & 1 & 2 \\
                             \end{bmatrix}
                             \begin{bmatrix}
                                1 \\ 0 \\ 2 \\ 1 \\ 3 \\ 5
                             \end{bmatrix} \\
                          &= \begin{bmatrix}
                                6 & 3 & 6 \\
                                3 & 3 & 3 \\
                                6 & 3 & 10
                             \end{bmatrix}^{-1}
                             \begin{bmatrix}
                                12 \\ 9 \\ 17
                             \end{bmatrix} \\
                          &= \frac{1}{12}\begin{bmatrix}
                                7 & -4 & -3 \\
                                -4 & 8 & 0 \\
                                -3 & 0 & 3
                             \end{bmatrix}
                             \begin{bmatrix}
                                12 \\ 9 \\ 17
                             \end{bmatrix} \\
                          &= \frac{1}{12}\begin{bmatrix}
                                -3 \\ 24 \\ 15
                             \end{bmatrix} \\
                          &= \begin{bmatrix}
                                -\cfrac{1}{4} \\ 2 \\ \cfrac{5}{4}
                             \end{bmatrix}
                \end{align*}
                
                Therefore, $\hat{\beta}_1 = 2$, $\hat{\beta_2} = 5/4$
            }
            \item {
                \[
                    \mathbf{x} = \begin{bmatrix}
                        1 & 0 \\
                        1 & 0 \\
                        1 & 0 \\
                        1 & 1 \\
                        1 & 1 \\
                        1 & 1
                    \end{bmatrix},
                    \hat{\gamma} = \begin{bmatrix}
                        \hat{\gamma_0} \\ \hat{\gamma_1}
                    \end{bmatrix}
                \]
                
                And its coefficients:
                
                \begin{align*}
                    \hat{\gamma} &= (\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
                                 &= \left(\begin{bmatrix}
                                      1 & 1 & 1 & 1 & 1 & 1 \\
                                      0 & 0 & 0 & 1 & 1 & 1 \\
                                    \end{bmatrix}
                                    \begin{bmatrix}
                                      1 & 0 \\
                                      1 & 0 \\
                                      1 & 0 \\
                                      1 & 1 \\
                                      1 & 1 \\
                                      1 & 1
                                    \end{bmatrix}\right)^{-1}
                                    \begin{bmatrix}
                                      1 & 1 & 1 & 1 & 1 & 1 \\
                                      0 & 0 & 0 & 1 & 1 & 1 \\
                                    \end{bmatrix}
                                    \begin{bmatrix}
                                      1 \\ 0 \\ 2 \\ 1 \\ 3 \\ 5
                                    \end{bmatrix} \\
                                 &= \begin{bmatrix}
                                      1 \\ 2
                                    \end{bmatrix}
                \end{align*}
                
                Therefore, $\hat{\gamma_1} = 2 = \hat{\beta_1}$
            }
            \item {
                \begin{gather*}
                    \hat{u} = y - x_2 \delta = [I - (x_2 (x_2'x_2)^{-1}x_2')]y = [I - P_2] y \\
                    \hat{v} = x_1 - x_2 \delta = [I - (x_2 (x_2'x_2)^{-1}x_2')]x_1 = [I - P_2] x_1
                \end{gather*}
                
                where $P_2$ is an orthogonal projection matrix.
                
                Numerically, 
                \begin{gather*}
                    \hat{u} = 
                    \begin{bmatrix}
                       0.25 \\ 
                      -2.00 \\ 
                      -1.25 \\ 
                      0.25 \\ 
                      1.00 \\ 
                      1.75 \\ 
                    \end{bmatrix},
                    \hat{v} = 
                    \begin{bmatrix}
                       -0.50 \\ 
                      -0.50 \\ 
                      -0.50 \\ 
                      0.50 \\ 
                      0.50 \\ 
                      0.50 \\ 
                    \end{bmatrix}
                \end{gather*}
                
                Regress $\hat{u}$ on $\hat{v}$ with a linear model without intercept:
                
                \begin{align*}
                    \hat{\alpha} &= [\hat{v}'\hat{v}]^{-1}\hat{v}'\hat{u} \\
                                 &= 2
                \end{align*}
            
                Therefore, $\hat{\alpha} = 2 = \hat{\beta_1}$
            }
        \end{enumerate}
    }
```
```{=latex}
\item

\begin{enumerate}
\item
```
```{R problem4a}
X <- matrix(
    c(
        7, 4, 9, 0, 5,
        2, 6, 2, 9, 3,
        3, 7, 0, 0, 5
    ),
    ncol = 3
)

Y <- matrix(
    c(6, 2, 4, 2, 1)
)

X_intercept <- cbind(rep(1, 5), X)

beta_hat <- solve(t(X_intercept) %*% X_intercept) %*% t(X_intercept) %*% Y
beta_hat
```

```{=latex}
\item
```

```{R problem4b}
X_new <- c(1, 0, 4, 3)
Y_hat <- X_new %*% beta_hat
Y_hat
```
```{=latex}
\end{enumerate}

\item
\begin{enumerate}
\item
```
```{R problem5a}
data(mtcars)
mtcars

X_mtcars <- cbind(
    rep(1, length(mtcars$drat)),
    mtcars$wt,
    mtcars$hp,
    mtcars$qsec,
    mtcars$vs
)

beta_hat_mtcars <- solve(t(X_mtcars) %*% X_mtcars) %*% t(X_mtcars) %*% mtcars$drat
beta_hat_mtcars
```
```{=latex}
\item
```
```{R problem5b}
summary(lm(drat ~ wt + hp + qsec + vs, data = mtcars))
```
```{=latex}
\end{enumerate}
\end{enumerate}
```